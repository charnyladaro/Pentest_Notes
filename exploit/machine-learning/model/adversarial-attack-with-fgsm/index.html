<!DOCTYPE html>
<html lang="en" class="m-0 p-0 w-screen min-h-screen bg-navy-dark">
<!-- Mirrored from exploit-notes.hdks.org/exploit/machine-learning/model/adversarial-attack-with-fgsm/ by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 15 Apr 2024 05:46:49 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Adversarial Attack with FGSM (Fast Gradient Signed Method) | Exploit Notes</title>
        <meta name="description" content="Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack">
        
        <link rel="icon" type="image/x-icon" href="../../../../img/favicon.png">
        <link rel="stylesheet" href="../../../../pagefind/pagefind-ui.css"><link rel="stylesheet" href="../../../../styles.css">
        <link rel="stylesheet" href="../../../../css/carbonads.css">
        
        <script type="text/javascript" src="../../../../js/exploit.js"></script>
        

        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@hideckies">
        <meta name="twitter:creator" content="@hideckies">
        <meta property="og:url" content="https://exploit-notes.hdks.org/exploit/machine-learning/model/adversarial-attack-with-fgsm/">
        <meta property="og:title" content="Adversarial Attack with FGSM (Fast Gradient Signed Method) | Exploit Notes">
        <meta property="og:description" content="Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack">
        <meta property="og:image" content="https://exploit-notes.hdks.org/img/screenshot.png">
        
        <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RR6XYHYTQ9"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-RR6XYHYTQ9');
        </script>
    <script type="text/javascript" src="../../../../pagefind/pagefind-ui.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded',()=>{new PagefindUI({"element":"#search","showImages":false,"excerptLength":0,"showEmptyFilters":true,"showSubResults":false,"resetStyles":true,"bundlePath":"/pagefind/","baseUrl":"/"});});</script><link rel="stylesheet" href="../../../../codecopy/codecopy.css"><script type="text/javascript" src="../../../../codecopy/codecopy.js"></script></head>

    <body class="relative w-screen min-h-screen text-basic text-white text-sm">
        <header class="sticky top-0 left-0 w-full z-20 bg-gradient-to-r from-pink-dark p-2">
    <div class="md:mx-auto w-full md:w-2/3 h-full flex justify-around items-center">
        <div class="
                w-full md:w-2/3 lg:w-1/2 h-full flex flex-col md:flex-row items-center
                md:gap-x-3 gap-y-3 md:gap-y-0
            ">
            <a href="../../../../index.html">
                <span class="text-sm text-white font-bold">
                    Exploit Notes
                </span>
            </a>
            
            <div id="search" class=""></div>
        </div>

        
        <div class="hidden sm:flex w-full h-full items-center justify-center gap-x-3">

            
            <a href="https://github.com/hideckies/exploit-notes" target="_blank" rel="noopener noreferrer" alt="GitHub">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="w-7 h-7 fill-white">
                    <path d="
                            M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207
                            11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729
                            1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304
                            3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381
                            1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399
                            3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242
                            2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823
                            1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path>
                </svg>
            </a>

            
            <a href="https://twitter.com/hideckies" target="_blank" rel="noopener noreferrer" alt="Twitter">
                <img src="../../../../img/x.png" alt="Twitter" width="22" height="22" class="w-6 h-6">
            </a>

            
            <a href="https://ko-fi.com/hideckies" target="_blank" rel="noopener noreferrer" alt="Ko-fi">
                <img src="../../../../img/kofi_cropped.png" alt="Ko-fi" width="22" height="22" class="w-10 h-10">
            </a>

        </div>
    </div>
</header>


<div class="hamburger fixed top-3 right-2 z-40 block md:hidden cursor-pointer">
    <div class="lines">
        <span class="line bg-white"></span>
        <span class="line bg-white"></span>
        <span class="line bg-white"></span>
    </div>
    <div class="line-cross hidden md:hidden text-[22px] text-white leading-none"></div>
</div>

<div id="search_bg" class="fixed top-0 left-0 w-screen z-10 h-screen hidden bg-black opacity-50"></div>

<style>
.hamburger .lines span.line {
    display: block;
    width: 20px;
    height: 3px;
    margin-bottom: 5px;
}

.hamburger .line-cross:before {
    content: '\2716';
}
</style>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        //Display the black filter when searching
        const searchBgElem = document.getElementById("search_bg");
        const drawerElem = document.querySelector(".pagefind-ui__drawer");
        const inputElem = document.querySelector(".pagefind-ui__search-input");

        if (searchBgElem) {
            searchBgElem.addEventListener('click', () => {
                searchBgElem.classList.add("hidden");
                drawerElem.style.visibility = "hidden";
                drawerElem.style.opacity = 0;
            });
        }

        inputElem.addEventListener('focus', () => {
            if (searchBgElem) {
                searchBgElem.classList.remove("hidden");
                drawerElem.style.visibility = "visible";
                drawerElem.style.opacity = 1;
            }
        });
    });
</script>
        <nav class="nav-links hidden">
    <div class="w-full flex flex-col gap-y-4">
        <div>
            <h3 class="my-4 text-base text-white opacity-60">SOCIAL</h3>
            <div class="w-full flex flex-col gap-y-1">
                <a href="https://github.com/hideckies/exploit-notes" target="_blank" rel="noopener noreferrer" class="text-lg">
                    GitHub
                </a>
                <a href="https://twitter.com/hideckies" target="_blank" rel="noopener noreferrer" class="text-lg">
                    Twitter
                </a>
                <a href="https://ko-fi.com/hideckies" target="_blank" rel="noopener noreferrer" class="text-lg">
                    
                    Support Me
                </a>
            </div>
        </div>
        <div>
            <h3 class="my-4 text-base text-white opacity-60">PAGES</h3>
            <div class="w-full flex flex-col gap-y-1">
                <a href="../../../../disclaimer/index.html" class="text-lg">
                    Disclaimer
                </a>
            </div>
            <div class="w-full flex flex-col gap-y-1">
                <a href="../../../../privacy-policy/index.html" class="text-lg">
                    Privacy Policy
                </a>
            </div>
        </div>
        <div>
            <h3 class="my-4 text-base text-white opacity-60">OTHER TOOLS</h3>
            <div class="w-full flex flex-col gap-y-1">
                <a href="https://security-links.hdks.org/" target="_blank" rel="noopener noreferrer" class="text-lg">
                    Security Links
                </a>
                <a href="https://github.com/hideckies/hermit" target="_blank" rel="noopener noreferrer" class="text-lg">
                    Hermit C2
                </a>
                <a href="https://github.com/hideckies/fuzzagotchi" target="_blank" rel="noopener noreferrer" class="text-lg">
                    Fuzzagotchi
                </a>
            </div>
        </div>
    </div>
</nav>

<style>
    /* Navigation for mobile */
    .nav-links-hamburger {
        position: fixed;
        top: 0;
        right: 0;
        z-index: 30;
        width: 100vw;
        height: 100vh;
        min-height: 100vh;
        background-color: var(--color-slate-dark);
        padding: 60px 32px;
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        justify-content: flex-start;
        gap: 32px 0px;
        color: var(--color-khaki);
    }
</style>
        <main class="w-full h-full">
            <div class="block sm:grid grid-cols-[320px_minmax(400px,_1fr)_280px] py-2">

    
    <div id="exploit-leftside-inner" class="hidden sm:block w-full h-screen overflow-y-auto px-4 py-3">
        <div class="w-full">
            <div id="exploit-related">
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">
                        Data Processing
                    </h4>
                    
                        <div class="my-3">
                        
                            <a href="../../data-processing/cluster-analysis-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Cluster Analysis for Machine Learning
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../../data-processing/data-manipulation-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Data Manipulation for Machine Learning
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../../data-processing/dimensionality-reduction-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Dimensionality Reduction for Machine Learning
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">
                        Computer Vision
                    </h4>
                    
                        <div class="my-3">
                        
                            <a href="../../computer-vision/image-analysis-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Image Analysis for Machine Learning
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../../computer-vision/image-manipulation-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Image Manipulation for Machine Learning
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../../computer-vision/image-recognition-bypass-for-machine-learning/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Image Recognition Bypass for Machine Learning
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">
                        LLM
                    </h4>
                    
                        <div class="my-3">
                        
                            <a href="../../llm/adversarial-attack-on-nlp/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Adversarial Attack on NLP
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../../llm/llm-prompt-injection/index.html" class="text-sm text-pink-light hover:brightness-200">
                                LLM Prompt Injection
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">
                        Model
                    </h4>
                    
                        <div class="my-3">
                        
                            <span id="scroll-target" class="text-sm text-white">Adversarial Attack with FGSM (Fast Gradient Signed Method)</span>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../ml-model-analysis/index.html" class="text-sm text-pink-light hover:brightness-200">
                                ML Model Analysis
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="../model-inversion-attack/index.html" class="text-sm text-pink-light hover:brightness-200">
                                Model Inversion Attack
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">
                        Others
                    </h4>
                    

                    
                        
                            <div class="my-3">
                            
                                <a href="../../jupyter-notebook-pentesting/index.html" class="text-sm text-pink-light hover:brightness-200">
                                    Jupyter Notebook Pentesting
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="../../orange-data-mining/index.html" class="text-sm text-pink-light hover:brightness-200">
                                    Orange Data Mining
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="../../read-hdf5-file/index.html" class="text-sm text-pink-light hover:brightness-200">
                                    Read HDF5 (H5) File
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="../../read-pt/index.html" class="text-sm text-pink-light hover:brightness-200">
                                    Read PT File
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="../../read-qasm/index.html" class="text-sm text-pink-light hover:brightness-200">
                                    Read QASM
                                </a>
                            
                            </div>
                        
                    
                
            </div>
        </div>
    </div>
    

    <div id="exploit-center-inner" class="w-full h-screen overflow-y-auto px-2 md:px-6 py-3" data-pagefind-body="">
        <article>
            <div class="w-full p-4">
                <h1 id="exploit-title" class="text-4xl font-bold">Adversarial Attack with FGSM (Fast Gradient Signed Method)</h1>
                <p class="my-3 text-base">Last modified: 2023-08-22</p>

                
                <div class="my-3 flex flex-wrap items-end space-x-2 space-y-2">
                    
                    <span class="bg-pink-dark rounded-md text-base px-3 py-1 text-white">Machine Learning</span>
                    
                </div>
                
            </div>

            <hr class="border-1 border-white opacity-40">

            <p class="px-2 py-4 text-base">Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack</p>

            <div id="exploit-content" class="px-2 py-4 text-base">
                <h2 id="create-adversarial-examples-against-resnet" tabindex="-1"><a href="#create-adversarial-examples-against-resnet" class="header-anchor">Create Adversarial Examples against ResNet</a></h2>
<p>Reference: <a href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html">PyTorch Docs</a></p>
<p>It's recommended to use an environment which is optimized to implement a machine learning model such as <strong>Google Colaboratory</strong>, <strong>Jupyter Notebook</strong>.</p>
<h3 id="1.-import-modules" tabindex="-1"><a href="#1.-import-modules" class="header-anchor">1. Import Modules</a></h3>
<pre class="codecopy_pre codecopy_pre_0"><code class="language-python hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, models, transforms
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
</code><button class="codecopy_copy" data-codecopy="0">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="2.-load-resnet-model" tabindex="-1"><a href="#2.-load-resnet-model" class="header-anchor">2. Load ResNet Model</a></h3>
<p>We load the <strong>ResNet50</strong> pretrained on <strong>ImageNet</strong>. It's no problem whether <strong>ResNet18</strong>, <strong>ResNet34</strong>, etc.</p>
<pre class="codecopy_pre codecopy_pre_1"><code class="language-python hljs">model = models.resnet50(pretrained=<span class="hljs-literal">True</span>)
model.<span class="hljs-built_in">eval</span>()

torch.manual_seed(<span class="hljs-number">42</span>)
use_cuda = <span class="hljs-literal">True</span>
device = <span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Device: "</span>, device)
</code><button class="codecopy_copy" data-codecopy="1">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="3.-load%2Fpreprocess-image" tabindex="-1"><a href="#3.-load%2Fpreprocess-image" class="header-anchor">3. Load/Preprocess Image</a></h3>
<p>We use the image of the fluffy samoyed dog.</p>
<pre class="codecopy_pre codecopy_pre_2"><code class="language-python hljs">wget https://github.com/pytorch/hub/raw/master/images/dog.jpg
</code><button class="codecopy_copy" data-codecopy="2">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then need to preprocess it.</p>
<pre class="codecopy_pre codecopy_pre_3"><code class="language-python hljs"><span class="hljs-comment"># Define a function which preprocesss the original image</span>
preprocess = transforms.Compose([
  transforms.Resize(<span class="hljs-number">256</span>),
  transforms.CenterCrop(<span class="hljs-number">224</span>),
  transforms.ToTensor(),
  transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]),
])

orig_img_tensor = preprocess(orig_img)

<span class="hljs-comment"># Prepend one dimension to the tensor for inference</span>
orig_img_batch = orig_img_tensor.unsqueeze(<span class="hljs-number">0</span>)

<span class="hljs-comment"># Attach device to the image and the model</span>
orig_img_batch = orig_img_batch.to(device)
model = model.to(device)
</code><button class="codecopy_copy" data-codecopy="3">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="4.-load-imagenet-classes" tabindex="-1"><a href="#4.-load-imagenet-classes" class="header-anchor">4. Load ImageNet Classes</a></h3>
<p>We use the ImageNet classes. The labels will be used for checking which label the original image and adversarial images are classfied by the model.</p>
<pre class="codecopy_pre codecopy_pre_4"><code class="language-python hljs">wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt
</code><button class="codecopy_copy" data-codecopy="4">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then read this text file and assign to labels.</p>
<pre class="codecopy_pre codecopy_pre_5"><code class="language-python hljs"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"imagenet_classes.txt"</span>, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> f:
  labels = [s.strip() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> f.readlines()]
</code><button class="codecopy_copy" data-codecopy="5">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="5.-initial-prediction" tabindex="-1"><a href="#5.-initial-prediction" class="header-anchor">5. Initial Prediction</a></h3>
<p>Before creating adversarial examples, we need to know the classes and probabilities of the original image by the ResNet model.</p>
<pre class="codecopy_pre codecopy_pre_6"><code class="language-python hljs">pred = model(orig_img_batch)
probs = F.softmax(pred[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
probs_top5, idx_top5 = torch.topk(probs, <span class="hljs-number">5</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"The top 5 labels of highly probabilies:"</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(probs_top5.size(<span class="hljs-number">0</span>)):
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{labels[idx_top5[i]]}</span>: <span class="hljs-subst">{probs_top5[i].item()*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>%"</span>)

<span class="hljs-comment"># Extract the top probability and index (target) for use in the next sections</span>
target_prob = probs_top5[<span class="hljs-number">0</span>]
target_idx = idx_top5[<span class="hljs-number">0</span>]
</code><button class="codecopy_copy" data-codecopy="6">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The top5 labels/accuracies should be such as below.</p>
<pre class="codecopy_pre codecopy_pre_7"><code class="language-txt hljs language-plaintext">The top 5 labels of highly probabilies:
Samoyed: 87.33%
Pomeranian: 3.03%
white wolf: 1.97%
keeshond: 1.11%
Eskimo dog: 0.92%
</code><button class="codecopy_copy" data-codecopy="7">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>As we imagine, the <strong>ResNet</strong> model predicted the original image as <strong><code>Samoyed</code></strong> with <strong><code>87.33%</code></strong> accuracy.</p>
<h3 id="6.-define-function-to-denormalize" tabindex="-1"><a href="#6.-define-function-to-denormalize" class="header-anchor">6. Define Function to Denormalize</a></h3>
<p>Create a function to denormalize an input image. Since the original image must be denormalized before FGSM process, this function is used to do that.</p>
<pre class="codecopy_pre codecopy_pre_8"><code class="language-python hljs"><span class="hljs-keyword">def</span> <span class="hljs-title function_">denorm</span>(<span class="hljs-params">batch, mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]</span>):
  <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(mean, <span class="hljs-built_in">list</span>):
    mean = torch.tensor(mean).to(device)
  <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(std, <span class="hljs-built_in">list</span>):
    std = torch.tensor(std).to(device)
  <span class="hljs-keyword">return</span> batch * std.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) + mean.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
</code><button class="codecopy_copy" data-codecopy="8">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="7.-calculate-perturbations" tabindex="-1"><a href="#7.-calculate-perturbations" class="header-anchor">7. Calculate Perturbations</a></h3>
<p>This process is the main role of the Adversarial Attack.<br>
It calculates the sign of the backpropagated gradients. It will be used for adjusting the input data to maximize the loss value in the next section.</p>
<pre class="codecopy_pre codecopy_pre_9"><code class="language-python hljs"><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_perturbations</span>(<span class="hljs-params">image, target</span>):
  image.requires_grad = <span class="hljs-literal">True</span>

  <span class="hljs-comment"># Predict the original image</span>
  pred = model(image)

  loss = F.nll_loss(pred, target)
  model.zero_grad()
  loss.backward()

  gradient = image.grad.data
  signed_grad = gradient.sign()
  <span class="hljs-keyword">return</span> signed_grad

perturbations = calc_perturbations(orig_img_batch, torch.tensor([target_idx]))
</code><button class="codecopy_copy" data-codecopy="9">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="8.-start-creating-adversarial-examples" tabindex="-1"><a href="#8.-start-creating-adversarial-examples" class="header-anchor">8. Start Creating Adversarial Examples</a></h3>
<p>Now generate adversarial exampels by each epsilon.<br>
The adversarial image is generated by adding the multiply of epsilong and perturbations to the original image data.<br>
Generally, the higher the value of <strong>epsilon</strong>, the less accuracy of the prediction by the model.</p>
<pre class="codecopy_pre codecopy_pre_10"><code class="language-python hljs">epsilons = [<span class="hljs-number">0</span>, <span class="hljs-number">.01</span>, <span class="hljs-number">.05</span>, <span class="hljs-number">.1</span>, <span class="hljs-number">.2</span>]

adv_examples = []

<span class="hljs-keyword">for</span> eps <span class="hljs-keyword">in</span> epsilons:
  orig_img_batch_denorm = denorm(orig_img_batch)
  adv_img = orig_img_batch_denorm + eps * perturbations
  adv_img = torch.clamp(adv_img, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)

  <span class="hljs-comment"># Normalize the adversarial image</span>
  adv_img_norm = transforms.Normalize((<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>), (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>))(adv_img)

  <span class="hljs-comment"># Predict the adversarial example</span>
  adv_pred = model(adv_img_norm)
  adv_probs = F.softmax(adv_pred[<span class="hljs-number">0</span>], dim=<span class="hljs-number">0</span>)
  adv_probs_top5, adv_idx_top5 = torch.topk(adv_probs, <span class="hljs-number">5</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">"-"</span>*<span class="hljs-number">28</span> + <span class="hljs-string">f"Eps <span class="hljs-subst">{eps}</span>"</span> + <span class="hljs-string">"-"</span>*<span class="hljs-number">28</span>)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(adv_probs_top5.size(<span class="hljs-number">0</span>)):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{labels[adv_idx_top5[i]]}</span>: <span class="hljs-subst">{adv_probs_top5[i]*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>%"</span>)
  <span class="hljs-built_in">print</span>()

  <span class="hljs-comment"># Make the adversarial example to the image to be saved</span>
  adv_ex = adv_img.squeeze().detach().cpu().numpy()

  adv_examples.append((labels[adv_idx_top5[<span class="hljs-number">0</span>]], adv_probs_top5[<span class="hljs-number">0</span>], adv_ex))
</code><button class="codecopy_copy" data-codecopy="10">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The output should be such as below.</p>
<pre class="codecopy_pre codecopy_pre_11"><code class="language-txt hljs language-plaintext">----------------------------Eps 0----------------------------
Samoyed: 87.33%
Pomeranian: 3.03%
white wolf: 1.97%
keeshond: 1.11%
Eskimo dog: 0.92%

----------------------------Eps 0.01----------------------------
West Highland white terrier: 43.36%
Scotch terrier: 8.47%
wallaby: 7.29%
cairn: 4.53%
Angora: 1.87%

----------------------------Eps 0.05----------------------------
West Highland white terrier: 92.15%
cairn: 1.28%
Angora: 1.16%
Scotch terrier: 1.06%
Maltese dog: 0.66%

----------------------------Eps 0.1----------------------------
West Highland white terrier: 97.47%
Scotch terrier: 0.57%
cairn: 0.31%
Angora: 0.17%
Maltese dog: 0.15%

----------------------------Eps 0.2----------------------------
West Highland white terrier: 50.01%
white wolf: 12.23%
ice bear: 8.72%
Arctic fox: 3.96%
Samoyed: 2.19%
</code><button class="codecopy_copy" data-codecopy="11">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>We should notice that adversarial images were not classified as <strong><code>Samoyed</code></strong>, but the other labels such as <strong><code>West Highland white terrier</code></strong> after the <strong>epsilon 0.01</strong>.</p>
<p>In short, we succeeded to fool the model’s predictions by modifying the original image.</p>
<h3 id="9.-plot-the-result" tabindex="-1"><a href="#9.-plot-the-result" class="header-anchor">9. Plot the Result</a></h3>
<p>Although this section is optional, we can plot the result above.</p>
<pre class="codecopy_pre codecopy_pre_12"><code class="language-python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

cnt = <span class="hljs-number">0</span>
plt.figure(figsize=(<span class="hljs-number">28</span>, <span class="hljs-number">10</span>))

<span class="hljs-keyword">for</span> i, eps <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(epsilons):
  cnt += <span class="hljs-number">1</span>
  plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(adv_examples), cnt)
  plt.xticks([])
  plt.yticks([])
  label, prob, img = adv_examples[i]
  plt.title(<span class="hljs-string">f"Eps <span class="hljs-subst">{eps}</span>\nClass: <span class="hljs-subst">{label}</span>\nAccuracy: <span class="hljs-subst">{prob*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>%"</span>, fontsize=<span class="hljs-number">14</span>)
  plt.imshow(img.T)
plt.show()
</code><button class="codecopy_copy" data-codecopy="12">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>We should see that the noise gets louder as the epsilon increases.<br>
However, from human eyes, these images are <strong><code>Samoyed</code></strong> no matter how you look at them.</p>
<h3 id="10.-save-the-adversarial-examples" tabindex="-1"><a href="#10.-save-the-adversarial-examples" class="header-anchor">10. Save the Adversarial Examples</a></h3>
<p>Finally, we save the generated adversarial images.<br>
Create new folder to store all adversarial images to be downloaded.</p>
<pre class="codecopy_pre codecopy_pre_13"><code class="language-bash hljs"><span class="hljs-built_in">mkdir</span> fake_dogs
</code><button class="codecopy_copy" data-codecopy="13">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Now save the images. We can use them to fool <strong>ResNet</strong> models.</p>
<pre class="codecopy_pre codecopy_pre_14"><code class="language-python hljs"><span class="hljs-comment"># Save adversarial images</span>
<span class="hljs-keyword">from</span> torchvision.utils <span class="hljs-keyword">import</span> save_image

<span class="hljs-keyword">for</span> i, eps <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(epsilons):
  label, prob, ex = adv_examples[i]
  ex_tensor = torch.from_numpy(ex).clone()
  save_image(ex_tensor, <span class="hljs-string">f"fake_dogs/fake_dog_eps<span class="hljs-subst">{eps}</span>.png"</span>)
</code><button class="codecopy_copy" data-codecopy="14">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<br>
<h2 id="create-adversarial-examples-against-mobilenetv2" tabindex="-1"><a href="#create-adversarial-examples-against-mobilenetv2" class="header-anchor">Create Adversarial Examples against MobileNetV2</a></h2>
<p>Reference: <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">TensorFlow Docs</a></p>
<h3 id="1.-load-pretrained-model-(mobilenetv2)" tabindex="-1"><a href="#1.-load-pretrained-model-(mobilenetv2)" class="header-anchor">1. Load Pretrained Model (MobileNetV2)</a></h3>
<pre class="codecopy_pre codecopy_pre_15"><code class="language-python hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

pretrained_model = tf.keras.applications.MobileNetV2(include_top=<span class="hljs-literal">True</span>, weights=<span class="hljs-string">'imagenet'</span>)
pretrained_model.trainable = <span class="hljs-literal">False</span>

<span class="hljs-comment"># ImageNet labels</span>
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions
</code><button class="codecopy_copy" data-codecopy="15">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="2.-prepare-original-image" tabindex="-1"><a href="#2.-prepare-original-image" class="header-anchor">2. Prepare Original Image</a></h3>
<p>We create functions to preprocess image and get label at first.</p>
<pre class="codecopy_pre codecopy_pre_16"><code class="language-python hljs"><span class="hljs-comment"># Helper function to preprocess the image so that it can be inputted in MobileNetV2</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">image</span>):
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
  image = image[<span class="hljs-literal">None</span>, ...]
  <span class="hljs-keyword">return</span> image

<span class="hljs-comment"># Helper function to extract labels from probability vector</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_imagenet_label</span>(<span class="hljs-params">probs</span>):
	<span class="hljs-keyword">return</span> decode_predictions(probs, top=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]
</code><button class="codecopy_copy" data-codecopy="16">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then load the original image and preprocess it.</p>
<pre class="codecopy_pre codecopy_pre_17"><code class="language-python hljs">orig_image_path = tf.keras.utils.get_file(<span class="hljs-string">'YellowLabradorLooking_new.jpg'</span>, <span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'</span>)
orig_image_raw = tf.io.read_file(image_path)
orig_image = tf.image.decode_image(image_raw)

orig_image = preprocess(image)
orig_image_probs = pretrained_model.predict(image)
</code><button class="codecopy_copy" data-codecopy="17">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>To get the label of the image that the model predicted, execute the following code.</p>
<pre class="codecopy_pre codecopy_pre_18"><code class="language-python hljs">_, orig_image_class, orig_class_confidence = get_imagenet_label(orig_image_probs)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f"class: <span class="hljs-subst">{orig_image_class}</span>"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"confidence: <span class="hljs-subst">{orig_class_confidence}</span>"</span>)

<span class="hljs-comment"># The output</span>
<span class="hljs-comment"># class: Labrador_retriever</span>
<span class="hljs-comment"># confidence: 0.418184757232666</span>
</code><button class="codecopy_copy" data-codecopy="18">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="3.-create-adversarial-image-with-fgsm" tabindex="-1"><a href="#3.-create-adversarial-image-with-fgsm" class="header-anchor">3. Create Adversarial Image with FGSM</a></h3>
<p>From this, we create the adversarial image to fool the MobileNetV2 model. The following code creates the perturbations to modify the original image.</p>
<pre class="codecopy_pre codecopy_pre_19"><code class="language-python hljs"><span class="hljs-comment"># Instantiate a function that computes the crossentropy loss between labels and predictions.</span>
loss_obj = tf.keras.losses.CategoricalCrossentropy()

<span class="hljs-keyword">def</span> <span class="hljs-title function_">create_adversarial_pattern</span>(<span class="hljs-params">input_image, input_label</span>):
	<span class="hljs-comment"># The gradient tape records the operations which are executed inside it.</span>
  <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_obj(input_label, prediction)

  <span class="hljs-comment"># Get the gradients of the loss w.r.t (with respect to) to the input image.</span>
  gradient = tape.gradient(loss, input_image)
  <span class="hljs-comment"># Get the sign of the gradients to create the perturbation.</span>
  signed_grad = tf.sign(gradient)
  <span class="hljs-keyword">return</span> signed_grad

<span class="hljs-comment"># The index of the label for labrador retriever</span>
target_label_idx = <span class="hljs-number">208</span>
orig_label = tf.one_hot(target_label_idx, orig_image_probs.shape[-<span class="hljs-number">1</span>])
orig_label = tf.reshape(orig_label, (<span class="hljs-number">1</span>, orig_image_probs.shape[-<span class="hljs-number">1</span>]))

perturbations = create_adversarial_pattern(orig_image, orig_label)
</code><button class="codecopy_copy" data-codecopy="19">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Now create adversarial examples and predict the labels by the classification model while increasing epsilon.</p>
<pre class="codecopy_pre codecopy_pre_20"><code class="language-python hljs"><span class="hljs-comment"># Epsilons are error terms (very small numbers)</span>
epsilons = [<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.15</span>]

<span class="hljs-keyword">for</span> i, eps <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(epsilons):
	adv_image = orig_image + eps*perturbations
	adv_image = tf.clip_by_value(adv_image, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
	<span class="hljs-comment"># Predict the label and the confidence for the adversarial image</span>
	_, label, confidence = get_imagenet_label(pretrained_model.predict(adv_image))
	<span class="hljs-built_in">print</span>(<span class="hljs-string">f"predicted label: <span class="hljs-subst">{label}</span>"</span>)
	<span class="hljs-built_in">print</span>(<span class="hljs-string">f"confidence: <span class="hljs-subst">{confidence*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f}</span>%"</span>)
	<span class="hljs-built_in">print</span>(<span class="hljs-string">"-"</span>*<span class="hljs-number">128</span>)
</code><button class="codecopy_copy" data-codecopy="20">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The outputs are something like below.</p>
<pre class="codecopy_pre codecopy_pre_21"><code class="language-txt hljs language-plaintext">1/1 [==============================] - 0s 25ms/step
predicted label: Labrador_retriever
confidence: 41.82%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 27ms/step
predicted label: Saluki
confidence: 13.08%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 24ms/step
predicted label: Weimaraner
confidence: 15.13%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 26ms/step
predicted label: Weimaraner
confidence: 16.58%
--------------------------------------------------------------------------------------------------------------------------------
</code><button class="codecopy_copy" data-codecopy="21">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>As above, the adversarial examples were predicted as different labels from the label that the original image was predicted (the original label is labrador retriever).<br>
To display the final adversarial image, execute the following code.</p>
<pre class="codecopy_pre codecopy_pre_22"><code class="language-python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

plt.imshow(adv_image[<span class="hljs-number">0</span>])
</code><button class="codecopy_copy" data-codecopy="22">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="4.-save%2Fload-the-adversarial-image" tabindex="-1"><a href="#4.-save%2Fload-the-adversarial-image" class="header-anchor">4. Save/Load the Adversarial Image</a></h3>
<p>We can save the generated adversarial image as below.</p>
<pre class="codecopy_pre codecopy_pre_23"><code class="language-python hljs">tf.keras.utils.save_img(<span class="hljs-string">"fake.png"</span>, adv_image[<span class="hljs-number">0</span>])
</code><button class="codecopy_copy" data-codecopy="23">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>To load this image, use Pillow.</p>
<pre class="codecopy_pre codecopy_pre_24"><code class="language-python hljs"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

fake_img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">"fake.png"</span>)
fake_img
</code><button class="codecopy_copy" data-codecopy="24">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<br>

            </div>

            
            <div class="my-6 p-4">
                <h2 class="text-3xl font-bold border-b-[1px] border-pink-dark">
                    References
                </h2>
                <ul class="my-6 ml-4 list-disc break-all">
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1412.6572" class="text-base text-pink-light hover:brightness-200">
                                https://arxiv.org/abs/1412.6572
                            </a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1810.00069" class="text-base text-pink-light hover:brightness-200">
                                https://arxiv.org/abs/1810.00069
                            </a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1804.00097" class="text-base text-pink-light hover:brightness-200">
                                https://arxiv.org/abs/1804.00097
                            </a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://tcode2k16.github.io/blog/posts/picoctf-2018-writeup/general-skills/#solution-20" class="text-base text-pink-light hover:brightness-200">
                                https://tcode2k16.github.io/blog/posts/picoctf-2018-writeup/general-skills/#solution-20
                            </a>
                        </li>
                    
                </ul>
            </div>
            
        </article>
    </div>

    
    <div id="exploit-rightside-inner" class="hidden sm:block w-full h-screen overflow-y-auto px-4 py-3">
        <div class="flex flex-col gap-y-4">
            <div>
                <h5 class="mb-4 border-b-2 border-slate-light text-sm text-white opacity-60">
                    ON THIS PAGE
                </h5>
                <div class="toc my-4 flex flex-col gap-y-2">
                <a href="#create-adversarial-examples-against-resnet" class="text-sm break-words">Create Adversarial Examples against ResNet</a><a href="#create-adversarial-examples-against-mobilenetv2" class="text-sm break-words">Create Adversarial Examples against MobileNetV2</a></div>
            </div>
            <div>
                
                <script async="" type="text/javascript" src="../../../../../cdn.carbonads.com/carbonebf3.js?serve=CWYDE53L&amp;placement=exploit-noteshdksorg" id="_carbonads_js">
                </script>
            </div>
        </div>
    </div>
    
</div>
        </main>
        <footer class="mt-12 w-full p-8 bg-navy-light">
    <div class="
            md:mx-auto w-full md:w-2/3
            flex flex-col md:flex-row md:items-start
            gap-y-3 md:gap-y-0 text-base
        ">

        <div class="w-full flex flex-col gap-y-2">
            <h3 class="text-sm opacity-60">SOCIALS</h3>
            <div class="w-full flex flex-col lg:flex-row items-start lg:items-center gap-y-2 lg:gap-3">
                <a href="https://github.com/hideckies/exploit-notes" target="_blank" rel="noopener noreferrer">
                    GitHub
                </a>            
                <a href="https://twitter.com/hideckies" target="_blank" rel="noopener noreferrer">
                    Twitter
                </a>
                <a href="https://ko-fi.com/hideckies" target="_blank" rel="noopener noreferrer">
                    Support Me
                </a>
            </div>
        </div>

        <div class="w-full flex flex-col gap-y-2">
            <h3 class="text-sm opacity-60">PAGES</h3>
            <div class="w-full flex flex-col lg:flex-row items-start lg:items-center gap-y-2 lg:gap-3">
                <a href="../../../../disclaimer/index.html">Disclaimer</a>
                <a href="../../../../privacy-policy/index.html">Privacy Policy</a>
            </div>
        </div>
        
        <div class="w-full flex flex-col gap-y-2">
            <h3 class="text-sm opacity-60">OTHER TOOLS</h3>
            <div class="w-full flex flex-col lg:flex-row lg:flex-wrap items-start lg:items-center gap-y-2 lg:gap-3">
                <a href="https://security-links.hdks.org/" target="_blank" rel="noopener noreferrer">
                    Security Links
                </a>
                <a href="https://github.com/hideckies/hermit" target="_blank" rel="noopener noreferrer">
                    Hermit C2
                </a>
                <a href="https://github.com/hideckies/fuzzagotchi" target="_blank" rel="noopener noreferrer">
                    Fuzzagotchi
                </a>
            </div>
        </div>

    </div>
</footer>
    

    <script type="text/javascript" src="../../../../js/hamburger.js"></script>

</body>
<!-- Mirrored from exploit-notes.hdks.org/exploit/machine-learning/model/adversarial-attack-with-fgsm/ by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 15 Apr 2024 05:46:49 GMT -->
</html>